{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import emojis\n",
    "from collections import defaultdict\n",
    "\n",
    "#export as html without code -> jupyter nbconvert Untitled.ipynb --no-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    '''Reads Whatsapp text file into a list of strings'''\n",
    "    x = open(file,'r', encoding = 'utf-8') #Opens the text file into variable x but the variable cannot be explored yet\n",
    "    y = x.read() #By now it becomes a huge chunk of string that we need to separate line by line\n",
    "    content = y.splitlines() #The splitline method converts the chunk of string into a list of strings\n",
    "    return content\n",
    "file_path = \"\" #complete here with your .txt path\n",
    "chat = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_patt = re.compile(r'(\\d+\\/\\d+\\/\\d+)\\,')\n",
    "time_patt = re.compile(r'\\s(\\d+\\:\\d+\\s[a-zA-z]{2})\\s\\-')\n",
    "name_patt = re.compile(r'\\-\\s([a-zA-Z0-9]+\\s?[a-zA-Z0-9]+\\s?[a-zA-Z0-9]+\\s?)\\:\\s')\n",
    "def split_line(line):\n",
    "    try:\n",
    "        date = date_patt.search(line).group(1)\n",
    "        time = time_patt.search(line).group(1)\n",
    "        name = name_patt.search(line).group(1)\n",
    "        msg = line.split(\":\")[2].lstrip().rstrip()\n",
    "    except Exception:\n",
    "        msg_data = {}\n",
    "        # some data is missing so... ignore that message\n",
    "    else:\n",
    "        format_ = \"%m/%d/%y\"\n",
    "        msg_data = {\n",
    "            \"date\":datetime.datetime.strptime(date, format_),\n",
    "            \"time\":time,\n",
    "            \"name\":name,\n",
    "            \"message\":msg\n",
    "        }\n",
    "    return msg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in chat:\n",
    "    data.append(split_line(line))\n",
    "data = [entry for entry in data if entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = df.name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whatsapp chat analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages sent per sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=\"name\")[\"name\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleted messages per sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['message'].str.contains(\"deleted\")].groupby(by=\"name\")[\"name\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media sent per sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['message'].str.contains(\"<Media omitted>\")].groupby(by=\"name\")[\"name\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_count_over_time = df.groupby(by=[\"date\", \"name\"]).name.agg(\"count\").to_frame(\"count_\").reset_index().sort_values(by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_count_over_time.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=(60,10))\n",
    "freq = 3\n",
    "fig, ax = plt.subplots(figsize=(30,10))\n",
    "for sender in senders:\n",
    "    tmp_df = messages_count_over_time[messages_count_over_time['name'] == sender]\n",
    "    \n",
    "    # Spot max count_ date\n",
    "    max_data = tmp_df.loc[tmp_df['count_'].idxmax()]\n",
    "    print(max_data.date, max_data.count_)\n",
    "    ax.annotate('Date: {}\\nCount: {}'.format(max_data.date, max_data.count_), xy=(max_data.date, max_data.count_))\n",
    "    \n",
    "    ax.plot(tmp_df.date, tmp_df.count_, label = sender)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.title(\"Messages sent over time\")\n",
    "    plt.xticks(tmp_df.date[::freq])\n",
    "\n",
    "plt.savefig(\"over_time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N emojis per sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(columnname, my_df):\n",
    "    # Credit \n",
    "    emojis=[]\n",
    "    for string in my_df[columnname]:\n",
    "        my_str = str(string)\n",
    "        for each in my_str:\n",
    "            if each in emoji.UNICODE_EMOJI:\n",
    "                emojis.append(each)\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_per_sender = {}\n",
    "for sender in senders:\n",
    "    emojis_count = defaultdict(int)\n",
    "    messages = df[df['name'] == sender][['name','message']]\n",
    "    emojis = extract_emojis(\"message\", messages)\n",
    "    \n",
    "    for e in emojis:\n",
    "        emojis_count[e] += 1    \n",
    "    \n",
    "    emojis_per_sender[sender] = emojis_count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "def pretty_print(dict_, n):\n",
    "    for key, value in dict_.items():\n",
    "        if n>=0:\n",
    "            print(key, \"--->\", value)\n",
    "            n-=1\n",
    "\n",
    "for sender, emojis in emojis_per_sender.items():\n",
    "    print(\"Top {} emojis sent by {}\".format(N, sender))\n",
    "    sorted_dict = {k: v for k, v in sorted(emojis.items(), key=lambda item: item[1], reverse=True)}\n",
    "    pretty_print(sorted_dict,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From deleted messages section and media sent section, we notice that we need to remove those expressions before building a corpus\n",
    "df = df[(df['message'] != \"<Media omitted>\") & (~df['message'].str.contains(\"deleted\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_string(string):\n",
    "    pattern = re.compile(r'(\\w+)')\n",
    "    try:\n",
    "        clean_string = pattern.search(string).group(1)\n",
    "        a,b = 'áéíóúü','aeiouu'\n",
    "        trans = str.maketrans(a,b)\n",
    "        clean_string = clean_string.translate(trans)\n",
    "    except AttributeError:\n",
    "        clean_string = \"\"\n",
    "    return clean_string    \n",
    "\n",
    "### From some analysis, you can notice some expression that might be removed (e.g. laughter expressions). \n",
    "### In this step, I'm removing those expressions and also some meaningless word (lenght =< 2)\n",
    "def build_vocab(corpus):\n",
    "    ## Tokenize and merge\n",
    "    vocab = list()\n",
    "    tmp = list()\n",
    "    for line in corpus:\n",
    "        tmp.extend(line.split(\" \"))\n",
    "    ## Normalize\n",
    "        # To lowercase\n",
    "    tmp = [word.lower() for word in tmp]\n",
    "        # Remove punctuation and replace accented chars\n",
    "    for w in tmp:\n",
    "        cs = clean_string(w)\n",
    "        if cs:\n",
    "            vocab.append(cs)\n",
    "    ## Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    vocab = [word for word in vocab if (\"jaja\" not in word) and (len(word)>2)]\n",
    "    return vocab\n",
    "\n",
    "def prepare_text(text):\n",
    "    ## Tokenize and merge\n",
    "    vocab = list()\n",
    "    tmp = list()\n",
    "    \n",
    "    tmp = text.split(\" \")\n",
    "    ## Normalize\n",
    "        # To lowercase\n",
    "    tmp = [word.lower() for word in tmp]\n",
    "        # Remove punctuation and replace accented chars\n",
    "    for w in tmp:\n",
    "        cs = clean_string(w)\n",
    "        if cs:\n",
    "            vocab.append(cs)\n",
    "    ## Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    ## Remove meaningless expressions\n",
    "    vocab = [word for word in vocab if (\"jaja\" not in word) and (len(word)>2)]\n",
    "    \n",
    "    return \" \".join(vocab)\n",
    "\n",
    "def generate_wordcloud(list_, sender, mask=False, mask_path=\"\"):\n",
    "    from wordcloud import WordCloud\n",
    "    #convert list to string\n",
    "    unique_string=(\" \").join(list_)\n",
    "    if mask:\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        wordcloud = WordCloud(width = 2000,mask=mask, height = 300, background_color=\"white\").generate(unique_string)\n",
    "    else:\n",
    "        wordcloud = WordCloud(width = 2000, height = 500, background_color=\"white\").generate(unique_string)\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"wordcloud_{}.png\".format(sender))\n",
    "    plt.show()    \n",
    "    plt.close()    \n",
    "    return\n",
    "\n",
    "def word_freq(vocab):\n",
    "    words = defaultdict(int)\n",
    "    for word in vocab:\n",
    "        words[word] += 1\n",
    "    words = {k: v for k, v in sorted(words.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N words by sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "for sender in senders:\n",
    "    tmp_df = df[df['name'] == sender][['name','message']]\n",
    "    tmp_corpus = tmp_df.message\n",
    "    vocab = build_vocab(tmp_corpus)\n",
    "    word_frequencies = word_freq(vocab)\n",
    "    print(\"Top {} words written by {}\".format(N, sender))\n",
    "    pretty_print(word_frequencies,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wordcloud per sender\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "for sender in senders:\n",
    "    print(\"WordCloud for {}\".format(sender))\n",
    "    tmp_df = df[df['name'] == sender][['name','message']]\n",
    "    tmp_corpus = tmp_df.message\n",
    "    vocab = build_vocab(tmp_corpus)\n",
    "    vocab = [word for word in vocab if \"jaja\" not in word]\n",
    "    mask = './wordcloud_shapes/mariposa.jpg'\n",
    "    generate_wordcloud(vocab, sender, True, mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud by date: What were you talking about on...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2020-05-04\"\n",
    "date_df = df[df['date'] == date][['date', 'name','message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date: {}\".format(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare messages\n",
    "date_df['message'] = date_df['message'].apply(lambda x: prepare_text(x))\n",
    "tmp_corpus = date_df.message\n",
    "vocab = build_vocab(tmp_corpus)\n",
    "generate_wordcloud(vocab, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare messages\n",
    "df['message'] = df['message'].apply(lambda x: prepare_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    # Credits: https://github.com/MaartenGr/soan/blob/master/whatsapp/topic.py\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def topics(df):\n",
    "    # Credits: https://github.com/MaartenGr/soan/blob/master/whatsapp/topic.py\n",
    "    # Create Topics\n",
    "    for user in df.name.unique():\n",
    "        print(\"#\" * len(user) + \"########\")\n",
    "        print(\"### \" + user + \" ###\")\n",
    "        print(\"#\" * len(user) + \"########\\n\")        \n",
    "        \n",
    "        data_samples = df[df.name == user].message\n",
    "        data_samples = data_samples.tolist()\n",
    "        # Extracting Features\n",
    "        tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "        tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "        # Fitting LDA\n",
    "        topic_model = LatentDirichletAllocation(n_components=5, max_iter=15,\n",
    "                                        learning_method='online',\n",
    "                                        learning_offset=50.,\n",
    "                                        random_state=0)\n",
    "        topic_model.fit(tf)\n",
    "        feature_names = tf_vectorizer.get_feature_names()\n",
    "        \n",
    "        \n",
    "        print(\"\\nTopics in LDA model:\")\n",
    "        print_top_words(topic_model, feature_names, 7)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
